<a href="https://colab.research.google.com/github/ayrna/ap-ts-rnn/blob/main/redes_recurrentes/time_series_data.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Utilizar Keras para construir y entrenar redes LSTM con series temporales

En este cuaderno, vamos a usar keras para construir una red LSTM que resuelva un problema de predicci√≥n en series temporales.

El problema que vamos a estudiar es el de predicci√≥n de pasajeros de l√≠neas a√©reas internacionales. Para este problema se nos da un a√±o y un mes y tenemos que predecir el n√∫mero de pasajeros de l√≠neas a√©reas internacionales en unidades de 1.000. En otras palabras, tenemos que responder a la pregunta: "Dado el n√∫mero de pasajeros (en unidades de miles) de este mes, ¬øcu√°l ser√° el n√∫mero de pasajeros del pr√≥ximo mes?".

Los datos van de enero de 1949 a diciembre de 1960 con 144 observaciones.


```python
import tensorflow as tf
from tensorflow import keras

import numpy
import matplotlib.pyplot as plt
import pandas as pd
import math

from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
```

El siguiente paso es cargar nuestro conjunto de datos de entrenamiento como un `DataFrame` de Pandas. A continuaci√≥n, podemos extraer la matriz `numpy` del `DataFrame` y convertir los valores enteros en valores de punto flotante, que son m√°s adecuados para el modelado con una red neuronal.


```python
# Cargar el conjunto de datos usando pandas
try:
    url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv"
    df = pd.read_csv(url)
except:
    df = pd.read_csv("./data/airline-passengers.csv")

# Dar el formato correcto a la fecha y usarlo como √≠ndice
df.Month = pd.to_datetime(df.Month)
df.set_index('Month', inplace=True)

# Mostrar los primeros datos
print(df.head())

# Usar matplotlib para mostrar la serie
df.plot()
plt.title("N√∫mero de pasajeros desde 1949-01 a 1960-12")
plt.ylabel("N√∫mero de pasajeros")
plt.show()

# Fijar la semilla para reproducibilidad
numpy.random.seed(7)
```

                Passengers
    Month                 
    1949-01-01         112
    1949-02-01         118
    1949-03-01         132
    1949-04-01         129
    1949-05-01         121



    
![png](output_5_1.png)
    


Las redes LSTM pueden ser sensibles a la escala de los datos de entrada. Puede ser buena idea rescalar los datos en el rango [0,1]. Para ello, vamos a usar `MinMaxScaler`.


```python
passengers = df['Passengers']
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(passengers.values.reshape(-1,1))
```

Despu√©s de modelar nuestros datos y estimar su rendimiento en el conjunto de datos de entrenamiento, tenemos que hacernos una idea de su comportamiento con datos no vistos. Con los datos de series temporales, la secuencia de valores es importante. Un m√©todo sencillo que podemos utilizar es dividir el conjunto de datos ordenados en conjuntos de datos de entrenamiento y otro de test. El c√≥digo siguiente calcula el √≠ndice del punto de divisi√≥n y separa los datos en conjuntos de datos de entrenamiento con el 67% de las observaciones que podemos utilizar para entrenar nuestro modelo, dejando el 33% restante para probar el modelo.


```python
train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
```

Ahora vamos a definir una funci√≥n que va a generar los datos para el entrenamiento de la LSTM.

La funci√≥n toma dos argumentos: el conjunto de datos, que es un array de NumPy que queremos convertir en un conjunto de datos, y el look_back, que es el n√∫mero de pasos de tiempo anteriores que se utilizar√°n como variables de entrada para predecir el siguiente per√≠odo de tiempo - en este caso, por defecto, 1.

Este valor predeterminado crear√° un conjunto de datos donde X es el n√∫mero de pasajeros en un momento dado (t) e Y es el n√∫mero de pasajeros en el siguiente momento (t + 1).

De esta forma, estamos siguiendo la estrategia `one-to-one`.


```python
# Convertir un array en un conjunto de datos
def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-look_back-1):
        a = dataset[i:(i+look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i + look_back, 0])
    return numpy.array(dataX), numpy.array(dataY)
```

Ahora podemos utilizar esta funci√≥n para preparar los conjuntos de datos de entrenamiento y test para el modelado.


```python
look_back = 1
# Preparar el conjunto de train
trainX, trainY = create_dataset(train, look_back)

# Preparar el conjunto de test
testX, testY = create_dataset(test, look_back)
```

Podr√≠amos haber hecho esto de forma m√°s f√°cil usando el m√©todo `shift()`.

La red LSTM espera que los datos de entrada (X) tenga una estructura 3D de la siguiente forma: `[samples, time steps, features]`. Los `samples` son el n√∫mero de instantes (filas) que vamos a usar para entrenar. En `time_steps` tendremos tantos como entradas reciba a la vez la red por cada instante de tiempo. En `features` vamos a tener las distintas variables que pudiera considerar la red.

En nuestro caso, el array es de `[samples, features]`, nos falta una dimensi√≥n. Adem√°s, `time_steps=1` (`one-to-one`) y `features=1` (univariante). Pero tenemos que convertir la X a 3D. Lo haremos usando `reshape()`.


```python
# Reshape para tener [samples, time steps, features]
trainX = trainX.reshape(trainX.shape[0],1,trainX.shape[1])
testX = testX.reshape(testX.shape[0],1,testX.shape[1])
```

Ahora ya podemos dise√±ar nuestra red LSTM para el problema de predicci√≥n.

Vamos a tener una capa visible con 1 entrada, una capa oculta con 4 bloques LSTM y una capa de salida que realizar√° una predicci√≥n. La funci√≥n de activaci√≥n sigmoide se va a usar para los bloques LSTM.

La opci√≥n `stateful=True` es importante. Hace que la memoria de la LSTM no se resetee despu√©s de cada batch. Obliga a especificar el `batch_size`, ya que as√≠ sabremos la memoria m√°xima.


```python
# Tama√±o de batch
batch_size=1

# Crear un modelo secuencial
model = Sequential()

# Crear una capa LSTM
model.add(InputLayer(batch_input_shape=(batch_size, look_back, 1)))
model.add(LSTM(10, stateful=True))

# Crear una capa densa
model.add(Dense(1))
```

Ahora compilamos y entrenamos durante 100 √©pocas con tama√±o de batch 1.

Para forzar a que el estado se resetee una vez hayamos pasado por toda la serie temporal, vamos a incluir un bucle para el n√∫mero de √©pocas, en lugar de usar la forma habitual.


```python
# Compilar el modelo
model.compile(optimizer="adam", loss='mean_squared_error')

# Entrenamiento habitual (no resetear√≠a el estado)
# model.fit(trainX, trainY, epochs=30, batch_size=1)

# Entrenar el modelo con reseteo expl√≠cito
for i in range(100):
  print('√âpoca %d'%(i))
  model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)

```

    √âpoca 0
    94/94 - 3s - 27ms/step - loss: 0.0152
    √âpoca 1
    94/94 - 0s - 2ms/step - loss: 0.0113
    √âpoca 2
    94/94 - 0s - 2ms/step - loss: 0.0095
    √âpoca 3
    94/94 - 0s - 2ms/step - loss: 0.0084
    √âpoca 4
    94/94 - 0s - 2ms/step - loss: 0.0077
    √âpoca 5
    94/94 - 0s - 2ms/step - loss: 0.0071
    √âpoca 6
    94/94 - 0s - 2ms/step - loss: 0.0066
    √âpoca 7
    94/94 - 0s - 2ms/step - loss: 0.0062
    √âpoca 8
    94/94 - 0s - 2ms/step - loss: 0.0058
    √âpoca 9
    94/94 - 0s - 2ms/step - loss: 0.0055
    √âpoca 10
    94/94 - 0s - 2ms/step - loss: 0.0052
    √âpoca 11
    94/94 - 0s - 2ms/step - loss: 0.0049
    √âpoca 12
    94/94 - 0s - 2ms/step - loss: 0.0047
    √âpoca 13
    94/94 - 0s - 2ms/step - loss: 0.0045
    √âpoca 14
    94/94 - 0s - 2ms/step - loss: 0.0043
    √âpoca 15
    94/94 - 0s - 2ms/step - loss: 0.0041
    √âpoca 16
    94/94 - 0s - 2ms/step - loss: 0.0039
    √âpoca 17
    94/94 - 0s - 2ms/step - loss: 0.0038
    √âpoca 18
    94/94 - 0s - 2ms/step - loss: 0.0037
    √âpoca 19
    94/94 - 0s - 2ms/step - loss: 0.0036
    √âpoca 20
    94/94 - 0s - 2ms/step - loss: 0.0035
    √âpoca 21
    94/94 - 0s - 2ms/step - loss: 0.0034
    √âpoca 22
    94/94 - 0s - 2ms/step - loss: 0.0033
    √âpoca 23
    94/94 - 0s - 2ms/step - loss: 0.0032
    √âpoca 24
    94/94 - 0s - 2ms/step - loss: 0.0031
    √âpoca 25
    94/94 - 0s - 2ms/step - loss: 0.0031
    √âpoca 26
    94/94 - 0s - 2ms/step - loss: 0.0030
    √âpoca 27
    94/94 - 0s - 2ms/step - loss: 0.0029
    √âpoca 28
    94/94 - 0s - 2ms/step - loss: 0.0029
    √âpoca 29
    94/94 - 0s - 2ms/step - loss: 0.0028
    √âpoca 30
    94/94 - 0s - 2ms/step - loss: 0.0027
    √âpoca 31
    94/94 - 0s - 2ms/step - loss: 0.0027
    √âpoca 32
    94/94 - 0s - 2ms/step - loss: 0.0026
    √âpoca 33
    94/94 - 0s - 2ms/step - loss: 0.0026
    √âpoca 34
    94/94 - 0s - 3ms/step - loss: 0.0025
    √âpoca 35
    94/94 - 0s - 3ms/step - loss: 0.0025
    √âpoca 36
    94/94 - 0s - 3ms/step - loss: 0.0024
    √âpoca 37
    94/94 - 0s - 3ms/step - loss: 0.0024
    √âpoca 38
    94/94 - 0s - 3ms/step - loss: 0.0023
    √âpoca 39
    94/94 - 0s - 3ms/step - loss: 0.0023
    √âpoca 40
    94/94 - 0s - 2ms/step - loss: 0.0023
    √âpoca 41
    94/94 - 0s - 2ms/step - loss: 0.0022
    √âpoca 42
    94/94 - 0s - 2ms/step - loss: 0.0022
    √âpoca 43
    94/94 - 0s - 2ms/step - loss: 0.0022
    √âpoca 44
    94/94 - 0s - 2ms/step - loss: 0.0022
    √âpoca 45
    94/94 - 0s - 2ms/step - loss: 0.0021
    √âpoca 46
    94/94 - 0s - 2ms/step - loss: 0.0021
    √âpoca 47
    94/94 - 0s - 2ms/step - loss: 0.0021
    √âpoca 48
    94/94 - 0s - 2ms/step - loss: 0.0021
    √âpoca 49
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 50
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 51
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 52
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 53
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 54
    94/94 - 0s - 2ms/step - loss: 0.0020
    √âpoca 55
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 56
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 57
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 58
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 59
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 60
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 61
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 62
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 63
    94/94 - 0s - 2ms/step - loss: 0.0019
    √âpoca 64
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 65
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 66
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 67
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 68
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 69
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 70
    94/94 - 0s - 3ms/step - loss: 0.0018
    √âpoca 71
    94/94 - 0s - 3ms/step - loss: 0.0018
    √âpoca 72
    94/94 - 0s - 3ms/step - loss: 0.0018
    √âpoca 73
    94/94 - 0s - 4ms/step - loss: 0.0018
    √âpoca 74
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 75
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 76
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 77
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 78
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 79
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 80
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 81
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 82
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 83
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 84
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 85
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 86
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 87
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 88
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 89
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 90
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 91
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 92
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 93
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 94
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 95
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 96
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 97
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 98
    94/94 - 0s - 2ms/step - loss: 0.0018
    √âpoca 99
    94/94 - 0s - 2ms/step - loss: 0.0017


Una vez ajustado el modelo, podemos estimar su rendimiento en los conjuntos de datos de entrenamiento y de test. Esto nos dar√° un punto de comparaci√≥n para los nuevos modelos.

Hay que tener en cuenta que invertimos las predicciones antes de calcular las puntuaciones de error para asegurarnos de que el rendimiento se presenta en las mismas unidades que los datos originales (miles de pasajeros al mes).


```python
# Realizar las predicciones
trainPredict = model.predict(trainX,batch_size=batch_size)
testPredict = model.predict(testX,batch_size=batch_size)
# Deshacer la normalizaci√≥n
trainPredict = scaler.inverse_transform(trainPredict)
trainYOriginal = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testYOriginal = scaler.inverse_transform([testY])
# Calcular los error
trainScore = math.sqrt(mean_squared_error(trainYOriginal[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testYOriginal[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
```

    [1m94/94[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 1ms/step
    [1m46/46[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m0s[0m 2ms/step
    Train Score: 22.68 RMSE
    Test Score: 46.37 RMSE


Por √∫ltimo, podemos generar predicciones utilizando el modelo para el conjunto de datos de entrenamiento y de test para obtener una indicaci√≥n visual de la habilidad del modelo.

Debido a la forma en que se prepar√≥ el conjunto de datos, debemos desplazar las predicciones para que se alineen en el eje x con el conjunto de datos original. Una vez preparados, los datos se representan, mostrando el conjunto de datos original en azul, las predicciones del conjunto de datos de entrenamiento en verde y las predicciones del conjunto de datos de prueba no visto en rojo.


```python
# Hacer un shift de las predicciones de entrenamiento para el plot
trainPredictPlot = numpy.empty_like(dataset)
trainPredictPlot[:, :] = numpy.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# Hacer un shift de las predicciones de test para el plot
testPredictPlot = numpy.empty_like(dataset)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict

# Representar las predicciones
plt.title("Plot baseline and predictions")
plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()


```


    
![png](output_24_0.png)
    


Como podemos ver, la red ha hecho un buen trabajo, aunque las predicciones empiezan a desviarse al final.

---



## Referencias

Este material se ha basado en la entrada ["Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras"](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) de Machine Learning Mastery. Es muy recomendable consultar la entrada completa.
